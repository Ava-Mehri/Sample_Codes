{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BGiFpLHhVmsv"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NJmNsMkrD2oV"
   },
   "outputs": [],
   "source": [
    "file = open('mdp_input.txt', 'r').read()\n",
    "#read the file line by line and store lines in a list\n",
    "for line in file:\n",
    "  lines = file.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GRKJPqI_EbxX"
   },
   "outputs": [],
   "source": [
    "#parse input file into MDP class\n",
    "N = int(lines[0].split()[1])\n",
    "M = int(lines[0].split()[0])\n",
    "\n",
    "in_wall = lines[1].split(',')\n",
    "n_wall = len(in_wall)\n",
    "wall = []\n",
    "for i in range(n_wall):\n",
    "  wall.append((int(in_wall[i].split()[1])-1, int(in_wall[i].split()[0])-1))\n",
    "\n",
    "in_goal = lines[2].split(',')\n",
    "n_goal = len(in_goal)\n",
    "goal = []\n",
    "for i in range(n_goal):\n",
    "  goal.append((int(in_goal[i].split()[1])-1, int(in_goal[i].split()[0])-1, int(in_goal[i].split()[2])))\n",
    "\n",
    "reward = float(lines[3])\n",
    "\n",
    "probs = lines[4].split()\n",
    "probs[0] = float(probs[0])\n",
    "probs[1] = float(probs[1])\n",
    "probs[2] = float(probs[2])\n",
    "probs[3] = float(probs[3])\n",
    "\n",
    "gama = float(lines[5])\n",
    "epsilon = float(lines[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mfjHnMIKZrYa"
   },
   "outputs": [],
   "source": [
    "#given an action, this function computes the utility value of the state resulted from this action\n",
    "def U_val(row_indx, column_indx, action):\n",
    "  if (action == 0):\n",
    "    p = row_indx+1\n",
    "    q = column_indx\n",
    "  if (action == 1):\n",
    "    p = row_indx\n",
    "    q = column_indx-1\n",
    "  if (action == 2):\n",
    "    p = row_indx-1\n",
    "    q = column_indx\n",
    "  if (action == 3):\n",
    "    p = row_indx\n",
    "    q = column_indx+1\n",
    "\n",
    "  if (p==-1) or (p==N) or (q==-1) or (q==M):\n",
    "    p = row_indx\n",
    "    q = column_indx\n",
    "\n",
    "  for n in range(len(wall)):\n",
    "    if (p,q) == wall[n]:\n",
    "      p = row_indx\n",
    "      q = column_indx\n",
    "  \n",
    "  return U_prime[p][q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kaxfx7U6UJa6"
   },
   "outputs": [],
   "source": [
    "#this function performs value iteration algorithm and returns the optimal policy\n",
    "def Val_iteration():\n",
    "  global U_prime\n",
    "  #initialize utility values table\n",
    "  U_prime = np.zeros((N,M))\n",
    "  for i in range(len(goal)):\n",
    "    U_prime[goal[i][0]][goal[i][1]] = goal[i][2]\n",
    "  for i in range(len(wall)):\n",
    "    U_prime[wall[i][0]][wall[i][1]] = 0\n",
    "\n",
    "  print(f'results for policy iteration algorithm:\\nplease note that this U table is upside down, when the iterations are done I will print out the U table corresponding to the map of the grid world')\n",
    "  for iteration in range (10):\n",
    "  #while (True):\n",
    "    U = U_prime\n",
    "    delta = 0\n",
    "    print(f'U = {U}')\n",
    "    #compute utility value for each state\n",
    "    for i in range(N):\n",
    "      for j in range(M):\n",
    "        up = probs[0]*U_val(i,j,0) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,2)\n",
    "        left = probs[0]*U_val(i,j,1) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,3)\n",
    "        down = probs[0]*U_val(i,j,2) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,0)\n",
    "        right = probs[0]*U_val(i,j,3) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,1)\n",
    "        U_prime[i][j] = reward + gama* max(up, left, down, right)\n",
    "        #again change the utility values of goal cells and wall cells to their initila values\n",
    "        for n in range(len(goal)):\n",
    "          U_prime[goal[n][0]][goal[n][1]] = goal[n][2]\n",
    "        for m in range(len(wall)):\n",
    "          U_prime[wall[m][0]][wall[m][1]] = 0\n",
    "      \n",
    "        if abs(U_prime[i][j]-U[i][j]) > delta:\n",
    "          delta = abs(U_prime[i][j]-U[i][j])\n",
    "\n",
    "    #if delta > epsilon*(1-gama)/gama:\n",
    "      #break\n",
    "\n",
    "  #this part just transforms actions from number form to letter form\n",
    "  action_list = ['U', 'L', 'D', 'R']\n",
    "  action_dict = dict((i,c) for i, c in enumerate(action_list))\n",
    "  pi_star = [[None for i in range(M)] for j in range(N)]\n",
    "  for i in range(N):\n",
    "    for j in range(M):\n",
    "      up = probs[0]*U_val(i,j,0) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,2)\n",
    "      left = probs[0]*U_val(i,j,1) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,3)\n",
    "      down = probs[0]*U_val(i,j,2) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,0)\n",
    "      right = probs[0]*U_val(i,j,3) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,1)\n",
    "      pi_star[i][j] = action_dict[np.argmax([up, left, down, right])]\n",
    "\n",
    "  for n in range(len(goal)):\n",
    "    pi_star[goal[n][0]][goal[n][1]] = goal[n][2]\n",
    "  for m in range(len(wall)):\n",
    "    pi_star[wall[m][0]][wall[m][1]] = 'w'\n",
    "\n",
    "  #print U table on the map of the grid world\n",
    "  print(f'U on the map=')\n",
    "  for i in range(N-1,-1,-1):\n",
    "    print(f'{U[i]}')\n",
    "  #print optimal policy on the map of the grid world\n",
    "  print(f'optimal policy on the map=')\n",
    "  for i in range(N-1,-1,-1):\n",
    "    print(f'{pi_star[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YbYzqPmOV6Nu"
   },
   "outputs": [],
   "source": [
    "#this function performs modified policy iteration algorithm and returns the optimal policy\n",
    "def Policy_iteration():\n",
    "  global U_prime\n",
    "  #initialize utility values table\n",
    "  U_prime = np.zeros((N,M))\n",
    "  for i in range(len(goal)):\n",
    "    U_prime[goal[i][0]][goal[i][1]] = goal[i][2]\n",
    "  for i in range(len(wall)):\n",
    "    U_prime[wall[i][0]][wall[i][1]] = 0\n",
    "\n",
    "  #initialize optimal policy table\n",
    "  Pi = np.random.randint(4, size=(N,M))\n",
    "  ## policy iteration\n",
    "  print(f'results for policy iteration algorithm:\\nplease note that this Pi table is upside down, when the iterations are done I will print out the Pi table corresponding to the map of the grid world')\n",
    "  while (True):\n",
    "    print(f'Pi = {Pi}')\n",
    "    #compute utility value for each state given a certain policy for each state\n",
    "    #performing some simplified value iteration steps, eq. 17.10 edition 3 of book\n",
    "    for i in range(N):\n",
    "      for j in range(M):\n",
    "        up = probs[0]*U_val(i,j,0) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,2)\n",
    "        left = probs[0]*U_val(i,j,1) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,3)\n",
    "        down = probs[0]*U_val(i,j,2) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,0)\n",
    "        right = probs[0]*U_val(i,j,3) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,1)\n",
    "        if (Pi[i][j] == 0):\n",
    "          U_prime[i][j] = reward + gama*up\n",
    "        if (Pi[i][j] == 1):\n",
    "          U_prime[i][j] = reward + gama*left\n",
    "        if (Pi[i][j] == 2):\n",
    "          U_prime[i][j] = reward + gama*down\n",
    "        if (Pi[i][j] == 3):\n",
    "          U_prime[i][j] = reward + gama*right\n",
    "\n",
    "        #again change the utility values of goal cells and wall cells to their initila values\n",
    "        for n in range(len(goal)):\n",
    "          U_prime[goal[n][0]][goal[n][1]] = goal[n][2]\n",
    "        for m in range(len(wall)):\n",
    "          U_prime[wall[m][0]][wall[m][1]] = 0\n",
    "\n",
    "    unchanged = True\n",
    "    #find optimal policy for each state using the algorithm\n",
    "    for i in range(N):\n",
    "      for j in range(M):\n",
    "        up = probs[0]*U_val(i,j,0) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,2)\n",
    "        left = probs[0]*U_val(i,j,1) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,3)\n",
    "        down = probs[0]*U_val(i,j,2) + probs[1]*U_val(i,j,1) + probs[2]*U_val(i,j,3) + probs[3]*U_val(i,j,0)\n",
    "        right = probs[0]*U_val(i,j,3) + probs[1]*U_val(i,j,0) + probs[2]*U_val(i,j,2) + probs[3]*U_val(i,j,1)\n",
    "        if Pi[i][j] == 0:\n",
    "          SIG = up\n",
    "        if Pi[i][j] == 1:\n",
    "          SIG = left\n",
    "        if Pi[i][j] == 2:\n",
    "          SIG = down\n",
    "        if Pi[i][j] == 3:\n",
    "          SIG = right\n",
    "        if max(up, left, down, right) > SIG:\n",
    "          Pi[i][j] = np.argmax([up, left, down, right])\n",
    "          unchanged = False\n",
    "\n",
    "    if unchanged:\n",
    "      break\n",
    "\n",
    "  #this part just transforms actions from number form to letter form\n",
    "  action_list = ['U', 'L', 'D', 'R']\n",
    "  action_dict = dict((i,c) for i, c in enumerate(action_list))\n",
    "  Pi = np.ndarray.tolist(Pi)\n",
    "  for i in range(N):\n",
    "    for j in range(M):\n",
    "      Pi[i][j] = action_dict[Pi[i][j]]\n",
    "  for n in range(len(goal)):\n",
    "    Pi[goal[n][0]][goal[n][1]] = goal[n][2]\n",
    "  for m in range(len(wall)):\n",
    "    Pi[wall[m][0]][wall[m][1]] = 'w'\n",
    "\n",
    "  #print optimal policy on the map of the grid world\n",
    "  print(f'optimal policy on the map=')\n",
    "  for i in range(N-1,-1,-1):\n",
    "    print(f'{Pi[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5P4-q2aU4KS",
    "outputId": "f272ca96-c456-425f-9cb9-226489e612c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for policy iteration algorithm:\n",
      "please note that this U table is upside down, when the iterations are done I will print out the U table corresponding to the map of the grid world\n",
      "U = [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0. -3.]\n",
      " [ 0.  0.  0.  0.  2.]]\n",
      "U = [[-0.04       -0.04       -0.04        0.6366      0.392888  ]\n",
      " [-0.04        0.          0.6366      1.          0.41839548]\n",
      " [-0.04        0.          0.392888    0.41839548 -3.        ]\n",
      " [-0.04       -0.04        0.22376384  1.35556362  2.        ]]\n",
      "U = [[-0.074      -0.074       0.443599    0.7111014   0.51250804]\n",
      " [-0.074       0.          0.7111014   1.          0.42906909]\n",
      " [-0.074       0.          0.51250804  0.67034644 -3.        ]\n",
      " [-0.074       0.10535941  0.94436637  1.49220235  2.        ]]\n",
      "U = [[-0.1029      0.24906732  0.54169848  0.72960755  0.53616719]\n",
      " [-0.1029      0.          0.72960755  1.          0.44606456]\n",
      " [-0.1029      0.          0.70271176  0.7794281  -3.        ]\n",
      " [ 0.0166079   0.62008023  1.11469924  1.51308859  2.        ]]\n",
      "U = [[ 0.11187278  0.37069641  0.56419415  0.73353071  0.54229059]\n",
      " [ 0.01858049  0.          0.747687    1.          0.45167309]\n",
      " [-0.04485827  0.          0.84397737  0.80563832 -3.        ]\n",
      " [ 0.37925328  0.82340912  1.15538775  1.51709179  2.        ]]\n",
      "U = [[ 0.22316209  0.40667041  0.57031078  0.73457112  0.54399527]\n",
      " [ 0.1149089   0.          0.76021449  1.          0.453309  ]\n",
      " [ 0.21026632  0.          0.88588101  0.8119223  -3.        ]\n",
      " [ 0.57002737  0.88564322  1.16513026  1.5179662   2.        ]]\n",
      "U = [[ 0.26527191  0.4169453   0.57482475  0.7350997   0.54463866]\n",
      " [ 0.15991942  0.          0.76415999  1.          0.45388555]\n",
      " [ 0.38336389  0.          0.89660186  0.81342817 -3.        ]\n",
      " [ 0.64327565  0.90284792  1.16746424  1.51816852  2.        ]]\n",
      "U = [[ 0.27966407  0.42176153  0.577962    0.73542106  0.54496088]\n",
      " [ 0.24787374  0.          0.76533793  1.          0.45415367]\n",
      " [ 0.4625993   0.          0.89922824  0.81378899 -3.        ]\n",
      " [ 0.66793596  0.90735983  1.16802346  1.51821639  2.        ]]\n",
      "U = [[ 0.29163856  0.42471362  0.57904124  0.73554018  0.54509206]\n",
      " [ 0.31670606  0.          0.76565291  1.          0.45426566]\n",
      " [ 0.49283833  0.          0.89986241  0.81387545 -3.        ]\n",
      " [ 0.6756705   0.90850712  1.16815744  1.51822781  2.        ]]\n",
      "U = [[ 0.30051455  0.42594936  0.57937059  0.73557932  0.54513935]\n",
      " [ 0.3489701   0.          0.76573481  1.          0.45430734]\n",
      " [ 0.50323846  0.          0.90001478  0.81389616 -3.        ]\n",
      " [ 0.6779921   0.90879327  1.16818955  1.51823054  2.        ]]\n",
      "U on the map=\n",
      "[0.67866849 0.90886375 1.16819724 1.51823119 2.        ]\n",
      "[ 0.50658517  0.          0.90005132  0.81390113 -3.        ]\n",
      "[0.36152707 0.         0.76575591 1.         0.45432165]\n",
      "[0.30485176 0.42638339 0.5794665  0.7355915  0.54515519]\n",
      "optimal policy on the map=\n",
      "['R', 'R', 'R', 'R', 2]\n",
      "['U', 'w', 'U', 'U', -3]\n",
      "['U', 'w', 'R', 1, 'D']\n",
      "['R', 'R', 'U', 'U', 'L']\n"
     ]
    }
   ],
   "source": [
    "Val_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flJED1ltW5Q6",
    "outputId": "8695d706-0926-4f15-8a14-76d4c9cb10af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for policy iteration algorithm:\n",
      "please note that this Pi table is upside down, when the iterations are done I will print out the Pi table corresponding to the map of the grid world\n",
      "Pi = [[2 1 0 1 0]\n",
      " [1 1 1 1 1]\n",
      " [2 2 1 1 1]\n",
      " [1 1 3 1 1]]\n",
      "Pi = [[1 1 3 0 0]\n",
      " [2 0 3 3 1]\n",
      " [2 2 3 2 0]\n",
      " [1 3 0 3 0]]\n",
      "Pi = [[1 3 3 0 1]\n",
      " [2 3 3 2 1]\n",
      " [0 3 2 0 0]\n",
      " [0 0 3 3 0]]\n",
      "Pi = [[3 3 3 0 1]\n",
      " [2 3 3 1 1]\n",
      " [0 3 0 0 0]\n",
      " [0 3 3 3 0]]\n",
      "Pi = [[3 3 3 0 1]\n",
      " [2 3 3 0 2]\n",
      " [0 3 0 0 0]\n",
      " [3 3 3 3 0]]\n",
      "optimal policy on the map=\n",
      "['R', 'R', 'R', 'R', 2]\n",
      "['U', 'w', 'U', 'U', -3]\n",
      "['D', 'w', 'R', 1, 'D']\n",
      "['R', 'R', 'R', 'U', 'L']\n"
     ]
    }
   ],
   "source": [
    "Policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
